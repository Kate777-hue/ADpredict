{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4VCnYCyKJNsvI4CwiCSJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kate777-hue/ADpredict/blob/main/6300_ad_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGepk-IPhLiL"
      },
      "source": [
        "#DATA 6300 Final Project Overview\n",
        "\n",
        "\n",
        "### Exploratory Data Analysis (EDA)\n",
        "- Analyze the relationships between features and the target variable (` Diagnosis`) to uncover meaningful patterns and trends.\n",
        "\n",
        "### Data Preprocessing\n",
        "- Remove irrelevant features, including PatientID and  DoctorInCharge.\n",
        "\n",
        "\n",
        "### Model Development\n",
        "- Train Baseline Models:\n",
        "  - Logistic Regression\n",
        "  - Support Vector Machine\n",
        "  - Random Forest\n",
        "  - XGBoost\n",
        "- Train Ensemble Models\n",
        "  - Voting\n",
        "  - Stacking\n",
        "- Analyze Feature Importance\n",
        "- Hyperparameter Tuning\n",
        "\n",
        "\n",
        "### Model Evaluation\n",
        "- Evaluate models using Accuracy, Precision, Recall, F1 Score.\n",
        "- Compare performance across models to identify the best balance between parameters and generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL_DwhiLhx0n"
      },
      "source": [
        "#Import Relevant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ydata-profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from matplotlib.pyplot import subplots\n",
        "import sklearn.model_selection as skm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "L5GYl1-wmSHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Data"
      ],
      "metadata": {
        "id": "uT3i4WsXm9Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data using google colab\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ERFFTAb_m-ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('alzheimers_disease_data.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "FzFatWRgnRHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I-decELkfma"
      },
      "source": [
        "#Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ProfileReport(df)"
      ],
      "metadata": {
        "id": "10MI8vVRnoVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the specified columns as irrelevant variables\n",
        "df.drop(['PatientID', 'DoctorInCharge'], axis=1, inplace=True)\n",
        "\n",
        "# Display the first few rows to verify the columns are dropped\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "hprcgovGpbc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpi0aNrhCZuh"
      },
      "source": [
        "##Define Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Diagnosis', axis=1)\n",
        "y = df['Diagnosis']"
      ],
      "metadata": {
        "id": "2hPxm5DsqU1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8GFDX97Dvti"
      },
      "source": [
        "##Separate Numercial and categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a threshold: if a numeric column has fewer unique values than this, consider it categorical.\n",
        "unique_threshold = 10\n",
        "\n",
        "numerical_vars = []\n",
        "categorical_vars = []\n",
        "\n",
        "for col in X.columns:\n",
        "    # Check if the column's datatype is numeric\n",
        "    if pd.api.types.is_numeric_dtype(X[col]):\n",
        "        unique_count = X[col].nunique()\n",
        "        if unique_count < unique_threshold:\n",
        "            categorical_vars.append(col)\n",
        "        else:\n",
        "            numerical_vars.append(col)\n",
        "    else:\n",
        "        # If not numeric, we assume it's categorical\n",
        "        categorical_vars.append(col)\n",
        "\n",
        "# Print the lists and their counts\n",
        "print(\"Numerical Variables:\", numerical_vars)\n",
        "print(\"Count of Numerical Variables:\", len(numerical_vars))\n",
        "print(\"\\nCategorical Variables:\", categorical_vars)\n",
        "print(\"Count of Categorical Variables:\", len(categorical_vars))"
      ],
      "metadata": {
        "id": "l0e_TddWqhME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "ZKpMs0zMqpTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the training and testing data\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the numerical columns of the training set and transform them\n",
        "#scaler.fit(X_train[numerical_vars])\n",
        "#print(\"Mean:\", scaler.mean_)\n",
        "#print(\"Scale:\", scaler.scale_)\n",
        "#joblib.dump(scaler, \"scaler_model.pkl\")\n",
        "#X_train_scaled[numerical_vars] = scaler.transform(X_train[numerical_vars])\n",
        "X_train_scaled[numerical_vars] = scaler.fit_transform(X_train[numerical_vars])\n",
        "\n",
        "\n",
        "# Transform the numerical columns of the testing set using the fitted scaler\n",
        "X_test_scaled[numerical_vars] = scaler.fit_transform(X_test[numerical_vars])\n",
        "\n",
        "# Optionally, print a few rows to verify\n",
        "print(\"Scaled Training Data (first 5 rows):\")\n",
        "print(X_train_scaled.head())"
      ],
      "metadata": {
        "id": "L5RIo331q-JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine scaled features with the target for the training set\n",
        "train_data = X_train_scaled.copy()\n",
        "train_data['Diagnosis'] = y_train\n",
        "\n",
        "# Combine scaled features with the target for the test set\n",
        "test_data = X_test_scaled.copy()\n",
        "test_data['Diagnosis'] = y_test\n",
        "\n",
        "# Save the training and testing datasets as CSV files\n",
        "train_data.to_csv('train_dataset.csv', index=False)\n",
        "test_data.to_csv('test_dataset.csv', index=False)\n",
        "\n",
        "print(\"Training and testing datasets have been saved as 'train_dataset.csv' and 'test_dataset.csv'.\")\n"
      ],
      "metadata": {
        "id": "xRSozUY3rG3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  #load saved files\n",
        "\n",
        "files.download('train_dataset.csv')\n",
        "files.download('test_dataset.csv')"
      ],
      "metadata": {
        "id": "UZLAIScJrM_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndr9GDs6OnDT"
      },
      "source": [
        "#Modelling and Model Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #load train data\n",
        "# train_data = pd.read_csv('train_dataset.csv')\n",
        "# train_data.info()\n",
        "\n",
        "# #load test data\n",
        "# test_data = pd.read_csv('test_dataset.csv')\n",
        "# test_data.info()"
      ],
      "metadata": {
        "id": "_aIcs4fQrpQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLmPD8GVQLQc"
      },
      "source": [
        "##Train Baseline Models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Identify features and target ---\n",
        "\n",
        "X_train = train_data.drop('Diagnosis', axis=1)\n",
        "y_train = train_data['Diagnosis']\n",
        "\n",
        "X_test = test_data.drop('Diagnosis', axis=1)\n",
        "y_test = test_data['Diagnosis']"
      ],
      "metadata": {
        "id": "T1g5Gs_yr4rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Baseline Model 1: Logistic Regression ---\n",
        "print(\"\\n--- Logistic Regression ---\")\n",
        "lr = LogisticRegression(random_state=SEED, max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "MZiINR45r_r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Baseline Model 2: Support Vector Machines ---\n",
        "print(\"\\n--- Support Vector Machine ---\")\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=SEED)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_svm))"
      ],
      "metadata": {
        "id": "9pz8qM0lsCkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Baseline Model 3: Random Forest ---\n",
        "print(\"\\n--- Random Forest ---\")\n",
        "rf = RandomForestClassifier(random_state=SEED)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "o2J2KG1ysFfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Baseline Model 4: XGBoost ---\n",
        "print(\"\\n--- XGBoost ---\")\n",
        "xgb_model = XGBClassifier(random_state=SEED, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# --- Make predictions ---\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# --- Evaluate the model ---\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "id": "6cFy08ZasJG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjevMW1xV327"
      },
      "source": [
        "##Ensemble Model 1: Voting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Voting Classifier 1\n",
        "# with Logistic Regression, Random Forest and XGBOOST\n",
        "\n",
        "voting_ensemble_1 = VotingClassifier(\n",
        "    estimators=[('lr', lr), ('rf', rf), ('xgb', xgb_model)],\n",
        "    voting='hard'  # or use 'soft' if models support predict_proba()\n",
        ")\n",
        "voting_ensemble_1.fit(X_train, y_train)\n",
        "y_pred_voting_1 = voting_ensemble_1.predict(X_test)\n",
        "\n",
        "print(\"Voting Ensemble 1 Accuracy:\", accuracy_score(y_test, y_pred_voting_1))\n",
        "print(\"Classification Report for Voting Ensemble 1:\")\n",
        "print(classification_report(y_test, y_pred_voting_1))\n",
        "print(\"Confusion Matrix for Voting Ensemble 1:\")\n",
        "print(confusion_matrix(y_test, y_pred_voting_1))\n",
        "\n",
        "\n",
        "# Create Voting Classifier 2\n",
        "# with Logistic Regression, SVM, Random Forest and XGBOOST\n",
        "\n",
        "voting_ensemble_2 = VotingClassifier(\n",
        "    estimators=[('lr', lr), ('svm', svm), ('rf', rf),('xgb', xgb_model)],\n",
        "    voting='hard'  # or use 'soft' if models support predict_proba()\n",
        ")\n",
        "voting_ensemble_2.fit(X_train, y_train)\n",
        "y_pred_voting_2 = voting_ensemble_2.predict(X_test)\n",
        "\n",
        "print(\"Voting Ensemble 2 Accuracy:\", accuracy_score(y_test, y_pred_voting_2))\n",
        "print(\"Classification Report for Voting Ensemble 2:\")\n",
        "print(classification_report(y_test, y_pred_voting_2))\n",
        "print(\"Confusion Matrix for Voting Ensemble 2:\")\n",
        "print(confusion_matrix(y_test, y_pred_voting_2))\n",
        "\n",
        "\n",
        "# Create Voting Classifier 3\n",
        "# with SVM, Random Forest and XGBOOST\n",
        "\n",
        "voting_ensemble_3 = VotingClassifier(\n",
        "    estimators=[('svm', svm), ('rf', rf),('xgb', xgb_model)],\n",
        "    voting='hard'  # or use 'soft' if models support predict_proba()\n",
        ")\n",
        "voting_ensemble_3.fit(X_train, y_train)\n",
        "y_pred_voting_3 = voting_ensemble_3.predict(X_test)\n",
        "\n",
        "print(\"Voting Ensemble 3 Accuracy:\", accuracy_score(y_test, y_pred_voting_3))\n",
        "print(\"Classification Report for Voting Ensemble 3:\")\n",
        "print(classification_report(y_test, y_pred_voting_3))\n",
        "print(\"Confusion Matrix for Voting Ensemble 3:\")\n",
        "print(confusion_matrix(y_test, y_pred_voting_3))\n",
        "\n"
      ],
      "metadata": {
        "id": "Bgz6Hg1rsf6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UagzKNdUZo2"
      },
      "source": [
        "##Ensemble Model 2: Stacking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Stacking Ensemble 1\n",
        "# with Logistic Regression, SVM, Random Forest and XGBOOST\n",
        "# Meta-model- Logistic Regression\n",
        "\n",
        "# Define base estimators (same as before)\n",
        "estimators = [('lr',lr), ('svm', svm), ('rf', rf), ('xgb', xgb_model)]\n",
        "\n",
        "# Define a meta-model for stacking (you can experiment with different models)\n",
        "meta_model = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "\n",
        "# Create Stacking Classifier ensemble\n",
        "stacking_ensemble_1 = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5  # 5-fold cross-validation for the meta-model\n",
        ")\n",
        "stacking_ensemble_1.fit(X_train, y_train)\n",
        "y_pred_stacking_1 = stacking_ensemble_1.predict(X_test)\n",
        "\n",
        "print(\"\\nStacking Ensemble 1 Accuracy:\", accuracy_score(y_test, y_pred_stacking_1))\n",
        "print(\"Classification Report for Stacking Ensemble 1:\")\n",
        "print(classification_report(y_test, y_pred_stacking_1))\n",
        "print(\"Confusion Matrix for Stacking Ensemble 1:\")\n",
        "print(confusion_matrix(y_test, y_pred_stacking_1))\n",
        "\n",
        "\n",
        "# Create Stacking Ensemble 2\n",
        "# with Logistic Regression, SVC, Random Forest and XGBOOST\n",
        "# Meta-model- SVC\n",
        "\n",
        "# Define base estimators (same as before)\n",
        "estimators = [('lr',lr), ('svm', svm), ('rf', rf), ('xgb', xgb_model)]\n",
        "\n",
        "# Define a meta-model for stacking (you can experiment with different models)\n",
        "meta_model = SVC(max_iter=1000, random_state=SEED)\n",
        "\n",
        "# Create Stacking Classifier ensemble\n",
        "stacking_ensemble_2 = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5  # 5-fold cross-validation for the meta-model\n",
        ")\n",
        "stacking_ensemble_2.fit(X_train, y_train)\n",
        "y_pred_stacking_2 = stacking_ensemble_2.predict(X_test)\n",
        "\n",
        "print(\"\\nStacking Ensemble 2 Accuracy:\", accuracy_score(y_test, y_pred_stacking_2))\n",
        "print(\"Classification Report for Stacking Ensemble 2:\")\n",
        "print(classification_report(y_test, y_pred_stacking_2))\n",
        "print(\"Confusion Matrix for Stacking Ensemble 2:\")\n",
        "print(confusion_matrix(y_test, y_pred_stacking_2))\n",
        "\n",
        "\n",
        "# Create Stacking Ensemble 3\n",
        "# with Logistic Regression, SVM, Random Forest and XGBOOST\n",
        "# Meta-model- Random Forest Classifier\n",
        "\n",
        "# Define base estimators (same as before)\n",
        "estimators = [('lr',lr), ('svm', svm), ('rf', rf), ('xgb', xgb_model)]\n",
        "\n",
        "# Define a meta-model for stacking (you can experiment with different models)\n",
        "meta_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create Stacking Classifier ensemble\n",
        "stacking_ensemble_3 = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5  # 5-fold cross-validation for the meta-model\n",
        ")\n",
        "stacking_ensemble_3.fit(X_train, y_train)\n",
        "y_pred_stacking_3 = stacking_ensemble_3.predict(X_test)\n",
        "\n",
        "print(\"\\nStacking Ensemble 3 Accuracy:\", accuracy_score(y_test, y_pred_stacking_3))\n",
        "print(\"Classification Report for Stacking Ensemble 3:\")\n",
        "print(classification_report(y_test, y_pred_stacking_3))\n",
        "print(\"Confusion Matrix for Stacking Ensemble 3:\")\n",
        "print(confusion_matrix(y_test, y_pred_stacking_3))\n",
        "\n",
        "\n",
        "# Create Stacking Ensemble 4\n",
        "# with Logistic Regression, Random Forest and XGBOOST\n",
        "# Meta-model- Logistic Regression\n",
        "\n",
        "estimators = [('lr', lr), ('rf', rf), ('xgb', xgb_model)]\n",
        "\n",
        "# Define a meta-model for stacking (you can experiment with different models)\n",
        "meta_model = LogisticRegression(random_state=SEED, max_iter=1000)\n",
        "\n",
        "# Create Stacking Classifier ensemble\n",
        "stacking_ensemble_4 = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5  # 5-fold cross-validation for the meta-model\n",
        ")\n",
        "stacking_ensemble_4.fit(X_train, y_train)\n",
        "y_pred_stacking_4 = stacking_ensemble_4.predict(X_test)\n",
        "\n",
        "print(\"\\nStacking Ensemble 4 Accuracy:\", accuracy_score(y_test, y_pred_stacking_4))\n",
        "print(\"Classification Report for Stacking Ensemble 4:\")\n",
        "print(classification_report(y_test, y_pred_stacking_4))\n",
        "print(\"Confusion Matrix for Stacking Ensemble 4:\")\n",
        "print(confusion_matrix(y_test, y_pred_stacking_4))"
      ],
      "metadata": {
        "id": "9h4D0UO8s8s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHULhu_QYRXz"
      },
      "source": [
        "##Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of model predictions\n",
        "model_predictions = {\n",
        "    \"Logistic Regression\": y_pred_lr,\n",
        "    \"SVM\": y_pred_svm,\n",
        "    \"Random Forest\": y_pred_rf,\n",
        "    \"XGBoost\": y_pred_xgb,\n",
        "    \"Voting Ensemble\": y_pred_voting_3,\n",
        "    \"Stacking Ensemble\": y_pred_stacking_3,\n",
        "\n",
        "}\n",
        "\n",
        "# Create a list to hold performance results for each model\n",
        "results = []\n",
        "\n",
        "# Compute evaluation metrics for each model\n",
        "for model_name, y_pred in model_predictions.items():\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1 Score\": f1\n",
        "    })\n",
        "\n",
        "# Convert results to a DataFrame for a neat tabular view\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Performance Comparison Table:\")\n",
        "print(results_df)\n",
        "\n",
        "# --- Visualization ---\n",
        "# We can visualize each metric in a separate bar plot.\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, metric in zip(axes, metrics):\n",
        "    sns.barplot(x=\"Model\", y=metric, data=results_df, ax=ax)\n",
        "    ax.set_title(metric)\n",
        "    ax.set_ylim(0, 1)  # Set limits between 0 and 1\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PuAsnSuktWEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlfNFEJCcYRM"
      },
      "source": [
        "##Analyze Feature Importance using gain metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analyzing feature importance\n",
        "\n",
        "xgb.plot_importance(xgb_model, max_num_features=10, importance_type='gain')\n",
        "plt.title(\"Feature Importance Using XGBoost\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ElorskFLtdDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWPDElLreH0A"
      },
      "source": [
        "##Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search Best Parameters for Baseline Models\n",
        "\n",
        "# Logistic Regression\n",
        "lr_params = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'solver': ['liblinear', 'lbfgs', 'saga']\n",
        "}\n",
        "\n",
        "# SVC\n",
        "svc_params = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Random Forest\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200, 300, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# XGBoost\n",
        "xgb_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "}\n"
      ],
      "metadata": {
        "id": "BE554Y-xuDN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform RandomizedSearchCV on each model\n",
        "lr_search = RandomizedSearchCV(LogisticRegression(random_state=SEED), lr_params, cv=5, n_iter=10, random_state=SEED, n_jobs=-1)\n",
        "svc_search = RandomizedSearchCV(SVC(probability=True, random_state=SEED), svc_params, cv=5, n_iter=10, random_state=SEED, n_jobs=-1)\n",
        "rf_search = RandomizedSearchCV(RandomForestClassifier(random_state=SEED), rf_params, cv=5, n_iter=10, random_state=SEED, n_jobs=-1)\n",
        "xgb_search = RandomizedSearchCV(XGBClassifier(random_state=SEED, eval_metric='mlogloss'), xgb_params, cv=5, n_iter=10, random_state=SEED, n_jobs=-1)\n",
        "\n",
        "# Fit models\n",
        "lr_search.fit(X_train, y_train)\n",
        "svc_search.fit(X_train, y_train)\n",
        "rf_search.fit(X_train, y_train)\n",
        "xgb_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best models\n",
        "best_lr = lr_search.best_estimator_\n",
        "best_svc = svc_search.best_estimator_\n",
        "best_rf = rf_search.best_estimator_\n",
        "best_xgb = xgb_search.best_estimator_\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Logistic Regression:\", lr_search.best_params_)\n",
        "print(\"Best SVC:\", svc_search.best_params_)\n",
        "print(\"Best Random Forest:\", rf_search.best_params_)\n",
        "print(\"Best XGBoost:\", xgb_search.best_params_)\n"
      ],
      "metadata": {
        "id": "aqqFsrBwuDBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the test set using tuned models\n",
        "# ------------------------------------------------------\n",
        "y_pred_tuned_lr = best_lr.predict(X_test)\n",
        "y_pred_tuned_svc = best_svc.predict(X_test)\n",
        "y_pred_tuned_rf = best_rf.predict(X_test)\n",
        "y_pred_tuned_xgb = best_xgb.predict(X_test)\n",
        "\n",
        "# --- Tuned Model Evaluations ---\n",
        "\n",
        "print(\"### Tuned Models ###\\n\")\n",
        "\n",
        "# Logistic Regression (Tuned)\n",
        "print(\"Tuned Logistic Regression:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned_lr))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned_lr))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned_lr))\n",
        "print(\"\\n------------------------\\n\")\n",
        "\n",
        "# Support Vector Classifier (Tuned)\n",
        "print(\"Tuned Support Vector Classifier:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned_svc))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned_svc))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned_svc))\n",
        "print(\"\\n------------------------\\n\")\n",
        "\n",
        "# Random Forest (Tuned)\n",
        "print(\"Tuned Random Forest:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned_rf))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned_rf))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned_rf))\n",
        "print(\"\\n------------------------\\n\")\n",
        "\n",
        "# XGBoost (Tuned)\n",
        "print(\"Tuned XGBoost:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned_xgb))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned_xgb))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned_xgb))"
      ],
      "metadata": {
        "id": "lqV6qrCzuOX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning\n",
        "# --- Voting Ensemble with tuned base models ---\n",
        "voting_ensemble_1 = VotingClassifier(\n",
        "    estimators=[('lr', best_lr), ('rf', best_rf), ('xgb', best_xgb)],\n",
        "    voting='hard'  # or 'soft'\n",
        ")\n",
        "voting_ensemble_1.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the tuned Voting1 Ensemble\n",
        "y_pred_tuned_voting_1 = voting_ensemble_1.predict(X_test)\n",
        "print(\"Voting Ensemble (tuned) Accuracy:\", accuracy_score(y_test, y_pred_tuned_voting_1))\n",
        "print(classification_report(y_test, y_pred_tuned_voting_1))\n",
        "\n",
        "\n",
        "voting_ensemble_2 = VotingClassifier(\n",
        "    estimators=[('lr', best_lr), ('svm', best_svc), ('rf', best_rf),('xgb', best_xgb)],\n",
        "    voting='hard'  # or use 'soft' if models support predict_proba()\n",
        ")\n",
        "voting_ensemble_2.fit(X_train, y_train)\n",
        "y_pred_voting_2 = voting_ensemble_2.predict(X_test)\n",
        "\n",
        "print(\"Voting Ensemble 2 Accuracy:\", accuracy_score(y_test, y_pred_voting_2))\n",
        "print(\"Classification Report for Voting Ensemble 2:\")\n",
        "print(classification_report(y_test, y_pred_voting_2))\n",
        "\n",
        "\n",
        "voting_ensemble_3 = VotingClassifier(\n",
        "    estimators=[('svm', best_svc), ('rf', best_rf),('xgb', best_xgb)],\n",
        "    voting='hard'  # or use 'soft' if models support predict_proba()\n",
        ")\n",
        "voting_ensemble_3.fit(X_train, y_train)\n",
        "y_pred_voting_3 = voting_ensemble_3.predict(X_test)\n",
        "\n",
        "print(\"Voting Ensemble 3 Accuracy:\", accuracy_score(y_test, y_pred_voting_3))\n",
        "print(\"Classification Report for Voting Ensemble 3:\")\n",
        "print(classification_report(y_test, y_pred_voting_3))\n"
      ],
      "metadata": {
        "id": "Qlf0CkzEuiNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Tuning\n",
        "# --- Stacking Ensemble with tuned base models ---\n",
        "\n",
        "# Tuned Stacking Ensemble 1\n",
        "stacking_ensemble_1= StackingClassifier(\n",
        "    estimators=[('lr', best_lr), ('rf', best_rf), ('xgb', best_xgb)],\n",
        "    final_estimator=LogisticRegression(random_state=SEED, max_iter=1000)\n",
        ")\n",
        "stacking_ensemble_1.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the tuned Stacking Ensemble\n",
        "y_pred_tuned_stack_1 = stacking_ensemble_1.predict(X_test)\n",
        "print(\"Stacking Ensemble (tuned) Accuracy:\", accuracy_score(y_test, y_pred_tuned_stack_1))\n",
        "print(classification_report(y_test, y_pred_tuned_stack_1))\n",
        "\n",
        "\n",
        "# Tuned Stacking Ensemble 2\n",
        "stacking_ensemble_2= StackingClassifier(\n",
        "    estimators=[('lr', best_lr), ('rf', best_rf), ('xgb', best_xgb)],\n",
        "    final_estimator=SVC(random_state=SEED, max_iter=1000)\n",
        ")\n",
        "stacking_ensemble_2.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the tuned Stacking Ensemble\n",
        "y_pred_tuned_stack_2 = stacking_ensemble_2.predict(X_test)\n",
        "print(\"Stacking Ensemble (tuned) Accuracy:\", accuracy_score(y_test, y_pred_tuned_stack_2))\n",
        "print(classification_report(y_test, y_pred_tuned_stack_2))\n",
        "\n",
        "\n",
        "# Tuned Stacking Ensemble 3\n",
        "stacking_ensemble_3= StackingClassifier(\n",
        "    estimators=[('lr', best_lr), ('rf', best_rf), ('xgb', best_xgb)],\n",
        "    final_estimator=RandomForestClassifier(random_state=SEED)\n",
        ")\n",
        "\n",
        "stacking_ensemble_3.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the tuned Stacking Ensemble\n",
        "y_pred_tuned_stack_3 = stacking_ensemble_3.predict(X_test)\n",
        "print(\"Stacking Ensemble (tuned) Accuracy:\", accuracy_score(y_test, y_pred_tuned_stack_3))\n",
        "print(classification_report(y_test, y_pred_tuned_stack_3))\n"
      ],
      "metadata": {
        "id": "x2yhbzpouqlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance"
      ],
      "metadata": {
        "id": "yiEbBdQZnP-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model performance with AUC/ROC\n",
        "\n",
        "# List of models\n",
        "models = {\n",
        "    \"Logistic Regression\": best_lr,\n",
        "    \"SVC\": best_svc,\n",
        "    \"Random Forest\": best_rf,\n",
        "    \"XGBoost\": best_xgb,\n",
        "    \"Voting Ensemble\": voting_ensemble_1,\n",
        "    \"Stacking Ensemble\": stacking_ensemble_1\n",
        "}"
      ],
      "metadata": {
        "id": "Vmhr9RiIuzmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Curve\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Check if the model has predict_proba\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        # Get probability scores\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]  # Select probabilities for positive class\n",
        "\n",
        "        # Compute ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot ROC curve\n",
        "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.2f})\")\n",
        "    else:\n",
        "        print(f\"Skipping ROC curve for {model_name} as it does not have predict_proba.\")\n",
        "\n",
        "# Plot random guess line\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guess\")\n",
        "\n",
        "# Labels & Legend\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve Comparison\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RT5JLooou4ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0V-8T_repUe"
      },
      "source": [
        "# Final Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary mapping model names to their predictions\n",
        "# ------------------------------\n",
        "model_predictions = {\n",
        "    \"LR (baseline)\": y_pred_lr,\n",
        "    \"SVM (baseline)\": y_pred_svm,\n",
        "    \"RF (baseline)\": y_pred_rf,\n",
        "    \"XGB (baseline)\": y_pred_xgb,\n",
        "    \"LR (tuned)\": y_pred_tuned_lr,\n",
        "    \"SVM (tuned)\": y_pred_tuned_svc,\n",
        "    \"RF (tuned)\": y_pred_tuned_rf,\n",
        "    \"XGB (tuned)\": y_pred_tuned_xgb,\n",
        "    \"Voting (ensemble)\": y_pred_voting_3,\n",
        "    \"Stacking (ensemble)\": y_pred_stacking_3,\n",
        "    \"Voting (tuned ensemble)\": y_pred_tuned_voting_1,\n",
        "    \"Stacking (tuned ensemble)\": y_pred_tuned_stack_1\n",
        "}\n",
        "\n",
        "# ------------------------------\n",
        "# Compute evaluation metrics for each model\n",
        "# ------------------------------\n",
        "results = []\n",
        "for model_name, y_pred in model_predictions.items():\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1 Score\": f1\n",
        "    })\n",
        "\n",
        "# Convert results to a DataFrame for a neat tabular view\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Performance Comparison Table:\")\n",
        "print(results_df)\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization of Metrics\n",
        "# ------------------------------\n",
        "# We'll create a 2x2 grid for the four metrics.\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, metric in zip(axes, metrics):\n",
        "    sns.barplot(x=\"Model\", y=metric, data=results_df, ax=ax, hue=\"Model\", palette=\"viridis\", dodge=False, legend=False) # Changed here\n",
        "    ax.set_title(metric, fontsize=14)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=12)\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YXxUASDmu-KS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}